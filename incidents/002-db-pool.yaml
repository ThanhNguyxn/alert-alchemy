# Incident 002: Database Connection Pool Exhaustion
# Difficulty: Medium-Hard | Optimal resolution: 2-3 steps

id: INC-002
title: "Database Connection Pool Exhaustion"
severity: critical
description: |
  The order-service is unable to acquire database connections.
  Connection pool (max 50) is fully exhausted. New requests are timing out
  waiting for connections. Checkout flow is completely broken.
  Revenue impact estimated at $5k/minute. Paged at 14:22 UTC during peak traffic.

services:
  - order-service
  - postgres-primary
  - payment-gateway

metrics:
  error_rate: 45.0
  p95_latency: 8500
  cpu_usage: 25.0
  memory_usage: 60.0
  request_count: 8000

logs:
  - "[14:22:01] ERROR order-service Connection pool exhausted (50/50 in use)"
  - "[14:22:03] ERROR order-service Timeout waiting for connection (30000ms)"
  - "[14:22:05] WARN  postgres-primary Active connections: 312 (limit 500)"
  - "[14:22:10] ERROR payment-gateway Upstream order-service timeout"
  - "[14:22:15] INFO  order-service Slow query detected: getOrderHistory (avg 4500ms)"
  - "[14:22:30] WARN  order-service 127 requests queued waiting for DB connection"

traces:
  - "trace-x1y2z3: POST /api/checkout -> order-service -> postgres (connection wait 28s)"
  - "trace-a9b8c7: GET /api/orders/history -> order-service (query time 4.2s per call)"
  - "trace-m3n4o5: Connection leak detected - transaction not closed in OrderHistoryDAO"

available_actions:
  - rollback
  - scale
  - restart
  - disable-flag
  - increase-pool
  - enable-cache

correct_action: increase-pool

timeline:
  - step: 0
    event: "Alert fired: order-service DB connections exhausted"

  - step: 1
    action: restart
    outcome: "Connections freed briefly, but exhaust again within 2 minutes."
    trade_off: "Causes request failures during restart. Not a fix."

  - step: 2
    action: scale
    outcome: "More pods = more connections needed. Makes problem worse."
    trade_off: "Each pod needs its own pool. Accelerates exhaustion."
    worsens: true

  - step: 3
    action: increase-pool
    outcome: "Pool increased to 100. Connections available, latency drops."
    trade_off: "Higher DB load. Need to investigate root cause after."
    resolved: true

  - step: 4
    action: disable-flag
    outcome: "If order-history feature disabled, slow query stops."
    trade_off: "Users can't see order history but checkout works."
    resolved: true

  - step: 5
    action: enable-cache
    outcome: "Cache reduces DB calls, frees connections over time."
    trade_off: "Takes 5-10 minutes to warm. Not immediate relief."

resolution:
  optimal_path: ["increase-pool"]
  alternate_path: ["disable-flag", "enable-cache"]
  explanation: |
    The root cause was a slow query in OrderHistoryDAO that held connections
    for 4+ seconds combined with a connection leak when exceptions occurred.
    Increasing pool size provides immediate relief. Long-term fix requires
    query optimization and fixing the leak. Disabling the order-history
    feature flag is a valid alternative that stops the problematic code path.
  post_mortem_link: "https://example.com/postmortems/INC-002"
